{"metadata":{"availableInstances":[{"_defaultOrder":0,"_isFastLaunch":true,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":4,"name":"ml.t3.medium","vcpuNum":2},{"_defaultOrder":1,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.t3.large","vcpuNum":2},{"_defaultOrder":2,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.t3.xlarge","vcpuNum":4},{"_defaultOrder":3,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.t3.2xlarge","vcpuNum":8},{"_defaultOrder":4,"_isFastLaunch":true,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.m5.large","vcpuNum":2},{"_defaultOrder":5,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.m5.xlarge","vcpuNum":4},{"_defaultOrder":6,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.m5.2xlarge","vcpuNum":8},{"_defaultOrder":7,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.m5.4xlarge","vcpuNum":16},{"_defaultOrder":8,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.m5.8xlarge","vcpuNum":32},{"_defaultOrder":9,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.m5.12xlarge","vcpuNum":48},{"_defaultOrder":10,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.m5.16xlarge","vcpuNum":64},{"_defaultOrder":11,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.m5.24xlarge","vcpuNum":96},{"_defaultOrder":12,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.m5d.large","vcpuNum":2},{"_defaultOrder":13,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.m5d.xlarge","vcpuNum":4},{"_defaultOrder":14,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.m5d.2xlarge","vcpuNum":8},{"_defaultOrder":15,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.m5d.4xlarge","vcpuNum":16},{"_defaultOrder":16,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.m5d.8xlarge","vcpuNum":32},{"_defaultOrder":17,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.m5d.12xlarge","vcpuNum":48},{"_defaultOrder":18,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.m5d.16xlarge","vcpuNum":64},{"_defaultOrder":19,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.m5d.24xlarge","vcpuNum":96},{"_defaultOrder":20,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":true,"memoryGiB":0,"name":"ml.geospatial.interactive","supportedImageNames":["sagemaker-geospatial-v1-0"],"vcpuNum":0},{"_defaultOrder":21,"_isFastLaunch":true,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":4,"name":"ml.c5.large","vcpuNum":2},{"_defaultOrder":22,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.c5.xlarge","vcpuNum":4},{"_defaultOrder":23,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.c5.2xlarge","vcpuNum":8},{"_defaultOrder":24,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.c5.4xlarge","vcpuNum":16},{"_defaultOrder":25,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":72,"name":"ml.c5.9xlarge","vcpuNum":36},{"_defaultOrder":26,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":96,"name":"ml.c5.12xlarge","vcpuNum":48},{"_defaultOrder":27,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":144,"name":"ml.c5.18xlarge","vcpuNum":72},{"_defaultOrder":28,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.c5.24xlarge","vcpuNum":96},{"_defaultOrder":29,"_isFastLaunch":true,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.g4dn.xlarge","vcpuNum":4},{"_defaultOrder":30,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.g4dn.2xlarge","vcpuNum":8},{"_defaultOrder":31,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.g4dn.4xlarge","vcpuNum":16},{"_defaultOrder":32,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.g4dn.8xlarge","vcpuNum":32},{"_defaultOrder":33,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.g4dn.12xlarge","vcpuNum":48},{"_defaultOrder":34,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.g4dn.16xlarge","vcpuNum":64},{"_defaultOrder":35,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":61,"name":"ml.p3.2xlarge","vcpuNum":8},{"_defaultOrder":36,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":244,"name":"ml.p3.8xlarge","vcpuNum":32},{"_defaultOrder":37,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":488,"name":"ml.p3.16xlarge","vcpuNum":64},{"_defaultOrder":38,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":768,"name":"ml.p3dn.24xlarge","vcpuNum":96},{"_defaultOrder":39,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.r5.large","vcpuNum":2},{"_defaultOrder":40,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.r5.xlarge","vcpuNum":4},{"_defaultOrder":41,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.r5.2xlarge","vcpuNum":8},{"_defaultOrder":42,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.r5.4xlarge","vcpuNum":16},{"_defaultOrder":43,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.r5.8xlarge","vcpuNum":32},{"_defaultOrder":44,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.r5.12xlarge","vcpuNum":48},{"_defaultOrder":45,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":512,"name":"ml.r5.16xlarge","vcpuNum":64},{"_defaultOrder":46,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":768,"name":"ml.r5.24xlarge","vcpuNum":96},{"_defaultOrder":47,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.g5.xlarge","vcpuNum":4},{"_defaultOrder":48,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.g5.2xlarge","vcpuNum":8},{"_defaultOrder":49,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.g5.4xlarge","vcpuNum":16},{"_defaultOrder":50,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.g5.8xlarge","vcpuNum":32},{"_defaultOrder":51,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.g5.16xlarge","vcpuNum":64},{"_defaultOrder":52,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.g5.12xlarge","vcpuNum":48},{"_defaultOrder":53,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.g5.24xlarge","vcpuNum":96},{"_defaultOrder":54,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":768,"name":"ml.g5.48xlarge","vcpuNum":192},{"_defaultOrder":55,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":1152,"name":"ml.p4d.24xlarge","vcpuNum":96},{"_defaultOrder":56,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":1152,"name":"ml.p4de.24xlarge","vcpuNum":96},{"_defaultOrder":57,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.trn1.2xlarge","vcpuNum":8},{"_defaultOrder":58,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":512,"name":"ml.trn1.32xlarge","vcpuNum":128},{"_defaultOrder":59,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":512,"name":"ml.trn1n.32xlarge","vcpuNum":128}],"instance_type":"ml.m5.2xlarge","kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Prompt Engineering: Cas d'utilisation de l'IA générative pour résumer des dialogues\n\nDans ce notebook, nous allons réaliser la tâche de résumé de dialogues en utilisant l'IA générative. \nNous verrons comment le texte d'entrée affecte la sortie d'un modèle de LLM, et effectuerons du Prompt Engineering pour l'orienter vers la tâche dont on a besoin.  \nEn comparant les techniques de zéro, one et few shot inference, nous verrons comment tout cela peut améliorer la sortie générative des LLMs.","metadata":{}},{"cell_type":"markdown","source":"# Table of Contents\n\n- [ 1 - Installation des Dépendances Requises](#1)\n- [ 2 - Résumer un Dialogue sans Ingénierie de Prompt](#2)\n- [ 3 - Résumer un Dialogue avec un Prompt d'Instruction](#3)\n  - [ 3.1 - Inférence Zero Shot avec un Instruction Prompt](#3.1)\n  - [ 3.2 - Inférence Zero Shot avec le Template de Prompt de FLAN-T5](#3.2)\n- [ 4 - Résumer un Dialogue avec One Shot et Few Shot Inference](#4)\n  - [ 4.1 - Inférence One Shot](#4.1)\n  - [ 4.2 - Inférence Few Shot](#4.2)\n- [ 5 - Paramètres de Configuration Générative pour l'Inférence](#5)","metadata":{}},{"cell_type":"markdown","source":"<a name='1'></a>\n## 1 - Installation des Dépendances Requises","metadata":{}},{"cell_type":"code","source":"!pip install datasets\n!pip install torch torchdata\n!pip install transformers","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-05-26T17:31:16.900963Z","iopub.execute_input":"2024-05-26T17:31:16.901362Z","iopub.status.idle":"2024-05-26T17:32:03.124964Z","shell.execute_reply.started":"2024-05-26T17:31:16.901330Z","shell.execute_reply":"2024-05-26T17:32:03.123472Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.18.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.22.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2+cpu)\nRequirement already satisfied: torchdata in /opt/conda/lib/python3.10/site-packages (0.7.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.2.0)\nRequirement already satisfied: urllib3>=1.25 in /opt/conda/lib/python3.10/site-packages (from torchdata) (1.26.18)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchdata) (2.31.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchdata) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchdata) (3.6)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchdata) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"# charger le dataset, le LLM, le tokenizer et le configurateur.\n\nfrom datasets import load_dataset\nfrom transformers import AutoModelForSeq2SeqLM\nfrom transformers import AutoTokenizer\nfrom transformers import GenerationConfig","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-05-26T17:34:41.066559Z","iopub.execute_input":"2024-05-26T17:34:41.066995Z","iopub.status.idle":"2024-05-26T17:34:48.503360Z","shell.execute_reply.started":"2024-05-26T17:34:41.066964Z","shell.execute_reply":"2024-05-26T17:34:48.502423Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"<a name='2'></a>\n## 2 - Résumer un Dialogue sans Prompt Engineering\n\nDans ce cas d'utilisation, on va générer un résumé d'un dialogue avec le modèle LLM pré-entraîné FLAN-T5 de Hugging Face.\n\nTéléchargeons quelques dialogues simples du dataset [DialogSum](https://huggingface.co/datasets/knkarthick/dialogsum) de Hugging Face. Ce dataset contient plus de 10 000 dialogues avec les résumés et les sujets correspondants étiquetés manuellement.","metadata":{}},{"cell_type":"code","source":"huggingface_dataset_name = \"knkarthick/dialogsum\"\n\ndataset = load_dataset(huggingface_dataset_name)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-05-26T17:34:48.505599Z","iopub.execute_input":"2024-05-26T17:34:48.506580Z","iopub.status.idle":"2024-05-26T17:34:52.489615Z","shell.execute_reply.started":"2024-05-26T17:34:48.506533Z","shell.execute_reply":"2024-05-26T17:34:52.488264Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/4.65k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ba72546d309437abadd2e37cfbd9a52"}},"metadata":{}},{"name":"stderr","text":"Downloading data: 100%|██████████| 11.3M/11.3M [00:00<00:00, 46.4MB/s]\nDownloading data: 100%|██████████| 442k/442k [00:00<00:00, 1.98MB/s]\nDownloading data: 100%|██████████| 1.35M/1.35M [00:00<00:00, 4.10MB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3eeff5beb66f4c80a765edc5e6b50d6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d69e29f209ce4b468c4fdb09cbd6c88e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04133779dec64344b849e6d7917b9c68"}},"metadata":{}}]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-26T17:34:52.491400Z","iopub.execute_input":"2024-05-26T17:34:52.492152Z","iopub.status.idle":"2024-05-26T17:34:52.502635Z","shell.execute_reply.started":"2024-05-26T17:34:52.492112Z","shell.execute_reply":"2024-05-26T17:34:52.501397Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 12460\n    })\n    validation: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 500\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 1500\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"dash_line = '-'.join('' for x in range(100))\nexample_indices = [202, 74]\n\nfor i, index in enumerate(example_indices):\n    print(dash_line)\n    print('Example ', i + 1)\n    \n    print(dash_line)\n    print('INPUT DIALOGUE:')\n    print(dataset['test'][index]['dialogue'])\n    \n    print(dash_line)\n    print('BASELINE HUMAN SUMMARY:')\n    print(dataset['test'][index]['summary'])\n    \n    print(dash_line)\n    print(\"\\n\\n\\n\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-05-26T17:55:27.864609Z","iopub.execute_input":"2024-05-26T17:55:27.864994Z","iopub.status.idle":"2024-05-26T17:55:27.874784Z","shell.execute_reply.started":"2024-05-26T17:55:27.864966Z","shell.execute_reply":"2024-05-26T17:55:27.873650Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nExample  1\n---------------------------------------------------------------------------------------------------\nINPUT DIALOGUE:\n#Person1#: Where to, miss?\n#Person2#: Hi! Crenshaw and Hawthorne, at the Holiday Inn that is on that corner.\n#Person1#: Sure thing. So, where are you flying in from?\n#Person2#: From China.\n#Person1#: Really? You don't look very Chinese to me, if you don't mind me saying so.\n#Person2#: It's fine. I am actually from Mexico. I was in China on a business trip, visiting some local companies that manufacture bathroom products.\n#Person1#: Wow sounds interesting! Excuse me if I am being a bit nosy but, how old are you?\n#Person2#: Don't you know it's rude to ask a lady her age?\n#Person1#: Don't get me wrong! It's just that you seem so young and already doing business overseas!\n#Person2#: Well thank you! In that case, I am 26 years old, and what about yourself?\n#Person1#: I am 40 years old and was born and raised here in the good old U. S of A, although I have some Colombian heritage.\n#Person2#: Really? That's great! Do you speak some Spanish?\n#Person1#: Uh. . . yeah. . of course!\n#Person2#: Que bien! Sentences poems habeas en espanol!\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# drives #Person2# to an inn and they have a talk. #Person2# is 26 and had a business trip to China. #Person1# is 40 years old American.\n---------------------------------------------------------------------------------------------------\n\n\n\n\n---------------------------------------------------------------------------------------------------\nExample  2\n---------------------------------------------------------------------------------------------------\nINPUT DIALOGUE:\n#Person1#: What makes you think you are able to do the job?\n#Person2#: My major is Automobile Designing and I have received my master's degree in science. I think I can do it well.\n#Person1#: What kind of work were you responsible for in the past employment?\n#Person2#: I am a student engineer who mainly took charge of understanding of the mechanical strength and corrosion resistance of various materials.\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# asks #Person2# about #Person2#'s qualification for the job.\n---------------------------------------------------------------------------------------------------\n\n\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"On charge le modèle [FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5), en créant une instance de la classe`AutoModelForSeq2SeqLM` avec la méthode`.from_pretrained()`. ","metadata":{}},{"cell_type":"code","source":"model_name='google/flan-t5-base'\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)","metadata":{"id":"iAYlS40Z3l-v","tags":[],"execution":{"iopub.status.busy":"2024-05-26T17:55:28.415340Z","iopub.execute_input":"2024-05-26T17:55:28.415727Z","iopub.status.idle":"2024-05-26T17:55:30.103915Z","shell.execute_reply.started":"2024-05-26T17:55:28.415699Z","shell.execute_reply":"2024-05-26T17:55:30.102773Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"On télécharge le tokenizer pour le modèle FLAN-T5 en utilisant la méthode `AutoTokenizer.from_pretrained()`.    \n\nLe paramètre `use_fast` active le tokenizer rapide - voir [documentation](https://huggingface.co/docs/transformers/v4.28.1/en/model_doc/auto#transformers.AutoTokenizer).","metadata":{"id":"sPqQA3TT3l_I","tags":[]}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)","metadata":{"id":"sPqQA3TT3l_I","tags":[],"execution":{"iopub.status.busy":"2024-05-26T17:55:30.105754Z","iopub.execute_input":"2024-05-26T17:55:30.106354Z","iopub.status.idle":"2024-05-26T17:55:30.485914Z","shell.execute_reply.started":"2024-05-26T17:55:30.106322Z","shell.execute_reply":"2024-05-26T17:55:30.484736Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"Testons l'encodage et le décodage d'une phrase simple par le tokenizer :","metadata":{"tags":[]}},{"cell_type":"code","source":"sentence = \"Burkina Fase is a west Africa country\"\n\nsentence_encoded = tokenizer(sentence, return_tensors='pt')\nprint('ENCODED SENTENCE:')\nprint(sentence_encoded[\"input_ids\"][0])","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-05-26T17:55:30.487064Z","iopub.execute_input":"2024-05-26T17:55:30.487441Z","iopub.status.idle":"2024-05-26T17:55:30.495745Z","shell.execute_reply.started":"2024-05-26T17:55:30.487410Z","shell.execute_reply":"2024-05-26T17:55:30.494312Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"ENCODED SENTENCE:\ntensor([4152, 2917,    9, 1699,    7,   15,   19,    3,    9, 4653, 2648,  684,\n           1])\n","output_type":"stream"}]},{"cell_type":"code","source":"sentence_decoded = tokenizer.decode(\n        sentence_encoded[\"input_ids\"][0], \n        skip_special_tokens=True\n    )\n\nprint('\\nDECODED SENTENCE:')\nprint(sentence_decoded)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T17:55:30.498311Z","iopub.execute_input":"2024-05-26T17:55:30.498731Z","iopub.status.idle":"2024-05-26T17:55:30.509035Z","shell.execute_reply.started":"2024-05-26T17:55:30.498700Z","shell.execute_reply":"2024-05-26T17:55:30.507917Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"\nDECODED SENTENCE:\nBurkina Fase is a west Africa country\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Il est maintenant temps d'explorer comment le **LLM de base** résume un dialogue sans Prompt Engineering.   \n\n**Le Prompt Engineering** est un acte par lequel un humain modifie le prompt** (entrée) afin d'améliorer la réponse pour une tâche donnée.","metadata":{}},{"cell_type":"code","source":"for i, index in enumerate(example_indices):\n    dialogue = dataset['test'][index]['dialogue']\n    summary = dataset['test'][index]['summary']\n    \n    inputs = tokenizer(dialogue, return_tensors='pt')\n    generation = model.generate( inputs[\"input_ids\"], \n                                 max_new_tokens=50,\n                                )[0]\n    \n    output = tokenizer.decode(generation, skip_special_tokens=True)\n    \n    print(dash_line)\n    print('Example ', i + 1)\n    \n    print(dash_line)\n    print(f'INPUT PROMPT:\\n{dialogue}')\n    \n    print(dash_line)\n    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n    \n    print(dash_line)\n    print(f'MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\\n{output}\\n')\n    print(\"\\n\\n\\n\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-05-26T17:55:30.510836Z","iopub.execute_input":"2024-05-26T17:55:30.511329Z","iopub.status.idle":"2024-05-26T17:55:34.635592Z","shell.execute_reply.started":"2024-05-26T17:55:30.511288Z","shell.execute_reply":"2024-05-26T17:55:34.634338Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nExample  1\n---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\n#Person1#: Where to, miss?\n#Person2#: Hi! Crenshaw and Hawthorne, at the Holiday Inn that is on that corner.\n#Person1#: Sure thing. So, where are you flying in from?\n#Person2#: From China.\n#Person1#: Really? You don't look very Chinese to me, if you don't mind me saying so.\n#Person2#: It's fine. I am actually from Mexico. I was in China on a business trip, visiting some local companies that manufacture bathroom products.\n#Person1#: Wow sounds interesting! Excuse me if I am being a bit nosy but, how old are you?\n#Person2#: Don't you know it's rude to ask a lady her age?\n#Person1#: Don't get me wrong! It's just that you seem so young and already doing business overseas!\n#Person2#: Well thank you! In that case, I am 26 years old, and what about yourself?\n#Person1#: I am 40 years old and was born and raised here in the good old U. S of A, although I have some Colombian heritage.\n#Person2#: Really? That's great! Do you speak some Spanish?\n#Person1#: Uh. . . yeah. . of course!\n#Person2#: Que bien! Sentences poems habeas en espanol!\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# drives #Person2# to an inn and they have a talk. #Person2# is 26 and had a business trip to China. #Person1# is 40 years old American.\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n#Person1#: Hi! #Person2#: Hi! I'm from Mexico. #Person1#: Okay, so I'm flying in from China. #Person2#: Okay, so I'\n\n\n\n\n\n---------------------------------------------------------------------------------------------------\nExample  2\n---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\n#Person1#: What makes you think you are able to do the job?\n#Person2#: My major is Automobile Designing and I have received my master's degree in science. I think I can do it well.\n#Person1#: What kind of work were you responsible for in the past employment?\n#Person2#: I am a student engineer who mainly took charge of understanding of the mechanical strength and corrosion resistance of various materials.\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# asks #Person2# about #Person2#'s qualification for the job.\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - WITHOUT PROMPT ENGINEERING:\nIn the past, I have worked on a number of projects.\n\n\n\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Vous pouvez voir que les suppositions du modèle ont un certain sens, mais il ne semble pas être sûr de la tâche qu'il est censé accomplir. On dirait qu'il se contente d'inventer la phrase suivante du dialogue.   \n\nLe Prompt Engineering peut être utile dans ce cas.","metadata":{}},{"cell_type":"markdown","source":"<a name='3'></a>\n## 3 - Résumer le dialogue à l'aide d'un Prompt d'instruction","metadata":{}},{"cell_type":"markdown","source":"<a name='3.1'></a>\n### 3.1 - Inférence Zero Shot avec un Instruction Prompt\n\nPour demander au modèle d'effectuer une tâche - résumer un dialogue - vous pouvez prendre le dialogue et le convertir en un instruction prompt.   \n\nEnveloppez le dialogue dans une instruction descriptive et voyez comment le texte généré va changer :","metadata":{}},{"cell_type":"code","source":"for i, index in enumerate(example_indices):\n    dialogue = dataset['test'][index]['dialogue']\n    summary = dataset['test'][index]['summary']\n\n    prompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary:\n    \"\"\"\n\n    # Input constructed prompt instead of the dialogue.\n    inputs = tokenizer(prompt, return_tensors='pt')\n    output = tokenizer.decode(\n        model.generate(\n            inputs[\"input_ids\"], \n            max_new_tokens=50,\n        )[0], \n        skip_special_tokens=True\n    )\n    \n    print(dash_line)\n    print('Example ', i + 1)\n    \n    print(dash_line)\n    print(f'INPUT PROMPT:\\n{prompt}')\n    \n    print(dash_line)\n    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n    \n    print(dash_line)    \n    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')\n    print(\"\\n\\n\\n\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-05-26T17:55:38.651903Z","iopub.execute_input":"2024-05-26T17:55:38.652655Z","iopub.status.idle":"2024-05-26T17:55:40.930830Z","shell.execute_reply.started":"2024-05-26T17:55:38.652622Z","shell.execute_reply":"2024-05-26T17:55:40.929670Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nExample  1\n---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\n\nSummarize the following conversation.\n\n#Person1#: Where to, miss?\n#Person2#: Hi! Crenshaw and Hawthorne, at the Holiday Inn that is on that corner.\n#Person1#: Sure thing. So, where are you flying in from?\n#Person2#: From China.\n#Person1#: Really? You don't look very Chinese to me, if you don't mind me saying so.\n#Person2#: It's fine. I am actually from Mexico. I was in China on a business trip, visiting some local companies that manufacture bathroom products.\n#Person1#: Wow sounds interesting! Excuse me if I am being a bit nosy but, how old are you?\n#Person2#: Don't you know it's rude to ask a lady her age?\n#Person1#: Don't get me wrong! It's just that you seem so young and already doing business overseas!\n#Person2#: Well thank you! In that case, I am 26 years old, and what about yourself?\n#Person1#: I am 40 years old and was born and raised here in the good old U. S of A, although I have some Colombian heritage.\n#Person2#: Really? That's great! Do you speak some Spanish?\n#Person1#: Uh. . . yeah. . of course!\n#Person2#: Que bien! Sentences poems habeas en espanol!\n\nSummary:\n    \n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# drives #Person2# to an inn and they have a talk. #Person2# is 26 and had a business trip to China. #Person1# is 40 years old American.\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\nThe flight is scheduled to arrive at the Holiday Inn in China.\n\n\n\n\n\n---------------------------------------------------------------------------------------------------\nExample  2\n---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\n\nSummarize the following conversation.\n\n#Person1#: What makes you think you are able to do the job?\n#Person2#: My major is Automobile Designing and I have received my master's degree in science. I think I can do it well.\n#Person1#: What kind of work were you responsible for in the past employment?\n#Person2#: I am a student engineer who mainly took charge of understanding of the mechanical strength and corrosion resistance of various materials.\n\nSummary:\n    \n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# asks #Person2# about #Person2#'s qualification for the job.\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\nObtain a job in the automotive industry.\n\n\n\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"C'est beaucoup mieux ! Mais le modèle ne saisit toujours pas les nuances des conversations.","metadata":{}},{"cell_type":"markdown","source":"<a name='3.2'></a>\n### 3.2 - Zero Shot Inference avec le Prompt Template de FLAN-T5\n\nUtilisons un prompt légèrement différent. FLAN-T5 dispose de nombreux modèles de prompt publiés pour certaines tâches [ici](https://github.com/google-research/FLAN/tree/main/flan/v2).    \n\nDans le code suivant, vous utiliserez un des [prompts pré-construits de FLAN-T5](https://github.com/google-research/FLAN/blob/main/flan/v2/templates.py) :","metadata":{}},{"cell_type":"code","source":"for i, index in enumerate(example_indices):\n    dialogue = dataset['test'][index]['dialogue']\n    summary = dataset['test'][index]['summary']\n        \n    prompt = f\"\"\"\nDialogue:\n\n{dialogue}\n\nWhat was going on?\n\"\"\"\n\n    inputs = tokenizer(prompt, return_tensors='pt')\n    output = tokenizer.decode(\n        model.generate(\n            inputs[\"input_ids\"], \n            max_new_tokens=50,\n        )[0], \n        skip_special_tokens=True\n    )\n\n    print(dash_line)\n    print('Example ', i + 1)\n    \n    print(dash_line)\n    print(f'INPUT PROMPT:\\n{prompt}')\n    \n    print(dash_line)\n    print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n    \n    print(dash_line)\n    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')\n    print(\"\\n\\n\\n\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-05-26T17:57:40.424429Z","iopub.execute_input":"2024-05-26T17:57:40.424835Z","iopub.status.idle":"2024-05-26T17:57:43.598238Z","shell.execute_reply.started":"2024-05-26T17:57:40.424805Z","shell.execute_reply":"2024-05-26T17:57:43.597030Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nExample  1\n---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\n\nDialogue:\n\n#Person1#: Where to, miss?\n#Person2#: Hi! Crenshaw and Hawthorne, at the Holiday Inn that is on that corner.\n#Person1#: Sure thing. So, where are you flying in from?\n#Person2#: From China.\n#Person1#: Really? You don't look very Chinese to me, if you don't mind me saying so.\n#Person2#: It's fine. I am actually from Mexico. I was in China on a business trip, visiting some local companies that manufacture bathroom products.\n#Person1#: Wow sounds interesting! Excuse me if I am being a bit nosy but, how old are you?\n#Person2#: Don't you know it's rude to ask a lady her age?\n#Person1#: Don't get me wrong! It's just that you seem so young and already doing business overseas!\n#Person2#: Well thank you! In that case, I am 26 years old, and what about yourself?\n#Person1#: I am 40 years old and was born and raised here in the good old U. S of A, although I have some Colombian heritage.\n#Person2#: Really? That's great! Do you speak some Spanish?\n#Person1#: Uh. . . yeah. . of course!\n#Person2#: Que bien! Sentences poems habeas en espanol!\n\nWhat was going on?\n\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# drives #Person2# to an inn and they have a talk. #Person2# is 26 and had a business trip to China. #Person1# is 40 years old American.\n\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\nCrenshaw and Hawthorne are flying in from China.\n\n\n\n\n\n---------------------------------------------------------------------------------------------------\nExample  2\n---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\n\nDialogue:\n\n#Person1#: What makes you think you are able to do the job?\n#Person2#: My major is Automobile Designing and I have received my master's degree in science. I think I can do it well.\n#Person1#: What kind of work were you responsible for in the past employment?\n#Person2#: I am a student engineer who mainly took charge of understanding of the mechanical strength and corrosion resistance of various materials.\n\nWhat was going on?\n\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# asks #Person2# about #Person2#'s qualification for the job.\n\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\nPerson1 is a student engineer who has a master's degree in automotive designing and a master's degree in science.\n\n\n\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Remarquez que cette invite de FLAN-T5 a aidé un peu, mais qu'elle a encore du mal à saisir les nuances de la conversation. C'est ce que vous allez essayer de résoudre avec le few shot inference.","metadata":{}},{"cell_type":"markdown","source":"<a name='4'></a>\n## 4 - Résumer un Dialogue avec One Shot et Few Shot Inference\n\n**One shot et few shot inference** sont les pratiques consistant à fournir à un LLM un ou plusieurs exemples complets de paires prompt-réponse correspondant à votre tâche - avant votre prompt réel que vous souhaitez compléter.    \n\nCela s'appelle \"l'apprentissage en contexte\" et place votre modèle dans un état qui comprend votre tâche spécifique. Vous pouvez en lire plus à ce sujet dans [ce blog de Hugging Face](https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api).","metadata":{}},{"cell_type":"markdown","source":"<a name='4.1'></a>\n### 4.1 - One Shot Inference\n\nConstruisons une fonction qui prend une liste de `example_indices_full`, génère un prompt avec des exemples complets, puis à la fin ajoute le prompt que vous souhaitez que le modèle complète (`example_index_to_summarize`).   \nVous utiliserez le même template de prompt FLAN-T5 de la section [3.2](#3.2).","metadata":{"tags":[]}},{"cell_type":"code","source":"def make_prompt(example_indices_full, example_index_to_summarize):\n    prompt = ''\n    for index in example_indices_full:\n        dialogue = dataset['test'][index]['dialogue']\n        summary = dataset['test'][index]['summary']\n        \n        # The stop sequence '{summary}\\n\\n\\n' is important for FLAN-T5. Other models may have their own preferred stop sequence.\n        prompt += f\"\"\"\nDialogue:\n\n{dialogue}\n\nWhat was going on?\n{summary}\n\n\n\"\"\"\n    \n    dialogue = dataset['test'][example_index_to_summarize]['dialogue']\n    \n    prompt += f\"\"\"\nDialogue:\n\n{dialogue}\n\nWhat was going on?\n\"\"\"\n        \n    return prompt","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-05-26T18:00:53.265011Z","iopub.execute_input":"2024-05-26T18:00:53.265509Z","iopub.status.idle":"2024-05-26T18:00:53.273563Z","shell.execute_reply.started":"2024-05-26T18:00:53.265476Z","shell.execute_reply":"2024-05-26T18:00:53.271991Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"Construct the prompt to perform one shot inference:","metadata":{"tags":[]}},{"cell_type":"code","source":"example_indices_full = [40]\nexample_index_to_summarize = 200\n\none_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n\nprint(one_shot_prompt)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-05-26T18:00:58.992606Z","iopub.execute_input":"2024-05-26T18:00:58.993022Z","iopub.status.idle":"2024-05-26T18:00:59.001168Z","shell.execute_reply.started":"2024-05-26T18:00:58.992991Z","shell.execute_reply":"2024-05-26T18:00:59.000067Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"\nDialogue:\n\n#Person1#: What time is it, Tom?\n#Person2#: Just a minute. It's ten to nine by my watch.\n#Person1#: Is it? I had no idea it was so late. I must be off now.\n#Person2#: What's the hurry?\n#Person1#: I must catch the nine-thirty train.\n#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n\nWhat was going on?\n#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n\n\n\nDialogue:\n\n#Person1#: Have you considered upgrading your system?\n#Person2#: Yes, but I'm not sure what exactly I would need.\n#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n#Person2#: That would be a definite bonus.\n#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n#Person2#: How can we do that?\n#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n#Person2#: No.\n#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n#Person2#: That sounds great. Thanks.\n\nWhat was going on?\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now pass this prompt to perform the one shot inference:","metadata":{"tags":[]}},{"cell_type":"code","source":"summary = dataset['test'][example_index_to_summarize]['summary']\n\ninputs = tokenizer(one_shot_prompt, return_tensors='pt')\noutput = tokenizer.decode(\n    model.generate(\n        inputs[\"input_ids\"],\n        max_new_tokens=50,\n    )[0], \n    skip_special_tokens=True\n)\n\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n\nprint(dash_line)\nprint(f'MODEL GENERATION - ONE SHOT:\\n{output}')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-05-26T18:01:13.311559Z","iopub.execute_input":"2024-05-26T18:01:13.312646Z","iopub.status.idle":"2024-05-26T18:01:16.001382Z","shell.execute_reply.started":"2024-05-26T18:01:13.312610Z","shell.execute_reply":"2024-05-26T18:01:16.000142Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ONE SHOT:\n#Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to add a CD-ROM drive.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name='4.2'></a>\n### 4.2 - Few Shot Inference\n\nExplorons le few shot inference en ajoutant deux autres paires complètes dialogue-résumé à votre prompt.","metadata":{"tags":[]}},{"cell_type":"code","source":"example_indices_full = [40, 80, 120]\nexample_index_to_summarize = 200\n\nfew_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n\nprint(few_shot_prompt)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-05-26T18:01:34.406871Z","iopub.execute_input":"2024-05-26T18:01:34.407275Z","iopub.status.idle":"2024-05-26T18:01:34.415929Z","shell.execute_reply.started":"2024-05-26T18:01:34.407247Z","shell.execute_reply":"2024-05-26T18:01:34.414596Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"\nDialogue:\n\n#Person1#: What time is it, Tom?\n#Person2#: Just a minute. It's ten to nine by my watch.\n#Person1#: Is it? I had no idea it was so late. I must be off now.\n#Person2#: What's the hurry?\n#Person1#: I must catch the nine-thirty train.\n#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n\nWhat was going on?\n#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n\n\n\nDialogue:\n\n#Person1#: May, do you mind helping me prepare for the picnic?\n#Person2#: Sure. Have you checked the weather report?\n#Person1#: Yes. It says it will be sunny all day. No sign of rain at all. This is your father's favorite sausage. Sandwiches for you and Daniel.\n#Person2#: No, thanks Mom. I'd like some toast and chicken wings.\n#Person1#: Okay. Please take some fruit salad and crackers for me.\n#Person2#: Done. Oh, don't forget to take napkins disposable plates, cups and picnic blanket.\n#Person1#: All set. May, can you help me take all these things to the living room?\n#Person2#: Yes, madam.\n#Person1#: Ask Daniel to give you a hand?\n#Person2#: No, mom, I can manage it by myself. His help just causes more trouble.\n\nWhat was going on?\nMom asks May to help to prepare for the picnic and May agrees.\n\n\n\nDialogue:\n\n#Person1#: Hello, I bought the pendant in your shop, just before. \n#Person2#: Yes. Thank you very much. \n#Person1#: Now I come back to the hotel and try to show it to my friend, the pendant is broken, I'm afraid. \n#Person2#: Oh, is it? \n#Person1#: Would you change it to a new one? \n#Person2#: Yes, certainly. You have the receipt? \n#Person1#: Yes, I do. \n#Person2#: Then would you kindly come to our shop with the receipt by 10 o'clock? We will replace it. \n#Person1#: Thank you so much. \n\nWhat was going on?\n#Person1# wants to change the broken pendant in #Person2#'s shop.\n\n\n\nDialogue:\n\n#Person1#: Have you considered upgrading your system?\n#Person2#: Yes, but I'm not sure what exactly I would need.\n#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n#Person2#: That would be a definite bonus.\n#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n#Person2#: How can we do that?\n#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n#Person2#: No.\n#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n#Person2#: That sounds great. Thanks.\n\nWhat was going on?\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now pass this prompt to perform a few shot inference:","metadata":{"tags":[]}},{"cell_type":"code","source":"summary = dataset['test'][example_index_to_summarize]['summary']\n\ninputs = tokenizer(few_shot_prompt, return_tensors='pt')\noutput = tokenizer.decode(\n    model.generate(\n        inputs[\"input_ids\"],\n        max_new_tokens=50,\n    )[0], \n    skip_special_tokens=True\n)\n\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'MODEL GENERATION - FEW SHOT:\\n{output}')","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dans ce cas, le few shot n'a pas apporté beaucoup d'amélioration par rapport au one shot inference. De plus, au-delà de 5 ou 6 shots, cela n'apportera généralement pas beaucoup d'améliorations non plus. Il faut également s'assurer de ne pas dépasser la longueur de contexte d'entrée du modèle qui, dans notre cas, est de 512 tokens. Tout ce qui dépasse la longueur de contexte sera ignoré.\n\nCependant, vous pouvez constater que fournir au moins un exemple complet (one shot) donne plus d'informations au modèle et améliore qualitativement le résumé global.","metadata":{"tags":[]}},{"cell_type":"markdown","source":"<a name='5'></a>\n## 5 - Paramètres de configuration générative pour l'inférence","metadata":{"tags":[]}},{"cell_type":"markdown","source":"Vous pouvez modifier les paramètres de configuration de la méthode `generate()` pour voir une sortie différente du LLM. Jusqu'à présent, le seul paramètre que vous avez défini était `max_new_tokens=50`, qui définit le nombre maximal de tokens à générer. Une liste complète des paramètres disponibles se trouve dans la [documentation sur la génération de Hugging Face](https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationConfig).\n\nUne façon pratique d'organiser les paramètres de configuration est d'utiliser la classe `GenerationConfig`.","metadata":{"tags":[]}},{"cell_type":"markdown","source":"**Exercice :**\n\nModifiez les paramètres de configuration pour étudier leur influence sur la sortie.\n\nEn activant le paramètre `do_sample = True`, vous activez diverses stratégies de décodage qui influencent le prochain token à partir de la distribution de probabilité sur l'ensemble du vocabulaire. Vous pouvez ensuite ajuster les sorties en modifiant `temperature` et d'autres paramètres (comme `top_k` et `top_p`).\n\nDécommentez les lignes dans la cellule ci-dessous et réexécutez le code. Essayez d'analyser les résultats. Vous pouvez lire quelques commentaires ci-dessous.","metadata":{"tags":[]}},{"cell_type":"code","source":"generation_config = GenerationConfig(max_new_tokens=50)\n# generation_config = GenerationConfig(max_new_tokens=10)\n# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.1)\n# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.5)\n# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=1.0)\n\ninputs = tokenizer(few_shot_prompt, return_tensors='pt')\noutput = tokenizer.decode(\n    model.generate(\n        inputs[\"input_ids\"],\n        generation_config=generation_config,\n    )[0], \n    skip_special_tokens=True\n)\n\nprint(dash_line)\nprint(f'MODEL GENERATION - FEW SHOT:\\n{output}')\n\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-05-26T18:03:35.861144Z","iopub.execute_input":"2024-05-26T18:03:35.861580Z","iopub.status.idle":"2024-05-26T18:03:39.809257Z","shell.execute_reply.started":"2024-05-26T18:03:35.861551Z","shell.execute_reply":"2024-05-26T18:03:39.808092Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (819 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nMODEL GENERATION - FEW SHOT:\n#Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to upgrade his hardware.\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Commentaires concernant le choix des paramètres dans la cellule de code ci-dessus :\n- Choisir `max_new_tokens=10` rendra le texte de sortie trop court, ce qui coupera le résumé du dialogue.\n- En mettant `do_sample = True` et en changeant la valeur de la température, vous obtiendrez plus de flexibilité dans la sortie.","metadata":{}},{"cell_type":"markdown","source":"Comme vous pouvez le constater, l'ingénierie des prompts peut vous emmener loin pour ce cas d'utilisation, mais il y a certaines limitations.     \n\nEnsuite, vous commencerez à explorer comment vous pouvez utiliser le fine-tuning pour aider votre LLM à mieux comprendre un cas d'utilisation particulier en profondeur !","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}