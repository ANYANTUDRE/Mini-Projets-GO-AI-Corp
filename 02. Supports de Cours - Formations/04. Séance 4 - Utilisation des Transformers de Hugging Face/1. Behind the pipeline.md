# Behind the pipeline

L'objectif de la biblioth√®que ü§ó Transformers est de fournir **une API** unique √† travers laquelle n'importe quel mod√®le de Transformer peut √™tre charg√©, entra√Æn√© et sauvegard√©.   
Ses principales caract√©ristiques sont :
- **Facilit√© d'utilisation,**
- **Flexibilit√©**,
- **Simplicit√©.**

Comme nous l'avons vu pr√©c√©demment, la fonction **pipeline** regroupe trois √©tapes : **pr√©-traitement**, passage des entr√©es √† travers le **mod√®le**, et **post-traitement** :
![](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg)


## Pr√©traitement avec un tokenizer

La premi√®re √©tape de notre pipeline consiste √† convertir les entr√©es textuelles brutes en nombres que le mod√®le peut interpr√©ter. Pour ce faire, nous utilisons un **tokenizer**, qui sera responsable de :

- **S√©parer l'entr√©e** en **tokens**,
- **Mettre en correspondance chaque token avec un nombre entier**,
- d'**ajouter des entr√©es suppl√©mentaires** qui peuvent √™tre utiles au mod√®le.

Tout ce pr√©traitement doit √™tre effectu√© exactement de la m√™me mani√®re que lorsque le mod√®le a √©t√© pr√©-entra√Æn√©, donc nous devons d'abord t√©l√©charger ces informations depuis le **Model Hub**. 
Pour ce faire, nous utilisons la classe **AutoTokenizer** et sa m√©thode `from_pretrained()`. 
En utilisant le nom du point de contr√¥le de notre mod√®le, elle va automatiquement r√©cup√©rer les donn√©es associ√©es au tokenizer du mod√®le et les mettre en cache.

Ex√©cutons ce qui suit pour l'analyse des sentiments :

```python
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```

Une fois que nous avons le tokenizer, nous pouvons directement lui passer nos phrases et nous obtiendrons en retour un dictionnaire pr√™t √† alimenter notre mod√®le ! La seule chose qu'il reste √† faire est de convertir la liste des identifiants d'entr√©e en tenseurs.

Vous pouvez utiliser ü§ó Transformers sans avoir √† vous soucier du framework ML utilis√© comme backend ; il peut s'agir de **PyTorch ou de TensorFlow, ou de Flax** pour certains mod√®les. Cependant, les mod√®les Transformer n'acceptent que des tenseurs en entr√©e.

```python
raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]

inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="tf")
print(inputs)
```
Voici les r√©sultats sous forme de **tenseurs TensorFlow** :
```
{
'input_ids': <tf.Tensor: shape=(2, 16), dtype=int32,
             numpy=array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,  2026,  2878,  2166,  1012,   102],
                          [ 101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,   0,     0,     0,     0,     0,     0,     0]], dtype=int32)>,

'attention_mask': <tf.Tensor: shape=(2, 16), dtype=int32,
            numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                         [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>
}
```

La sortie elle-m√™me est un dictionnaire contenant deux cl√©s, **input_ids** et **attention_mask**.   
- **input_ids:** contient deux lignes d'entiers (une pour chaque phrase) qui sont les identifiants uniques des tokens dans chaque phrase. 
- **attention_mask:** sera expliqu√© par la suite.


## Parcourir le mod√®le
Nous pouvons t√©l√©charger notre mod√®le pr√©-entra√Æn√© de la m√™me mani√®re que nous l'avons fait avec notre tokenizer. ü§ó Transformers fournit une classe **TFAutoModel** qui a aussi une m√©thode `from_pretrained` :
```python
from transformers import TFAutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = TFAutoModel.from_pretrained(checkpoint)
```

Cette architecture ne contient que le **module de base du transformer** : √©tant donn√© certaines entr√©es, il produit ce que nous appellerons des **√©tats cach√©s**( ou des **hidden states**), √©galement connus sous le nom de **features**. Pour chaque entr√©e du mod√®le, nous r√©cup√©rons un **high-dimensional vector** repr√©sentant la compr√©hension contextuelle de cette entr√©e par le mod√®le Transformer.
Bien que ces √©tats cach√©s puissent √™tre utiles en eux-m√™mes, ils sont g√©n√©ralement des entr√©es dans une autre partie du mod√®le, appel√©e **la t√™te**. Lorsqu'on a vu les exemples d'utilisation de la fonction pipeline, les diff√©rentes t√¢ches auraient pu √™tre ex√©cut√©es avec la m√™me architecture, mais chacune de ces t√¢ches sera associ√©e √† une t√™te diff√©rente.


### Un vecteur √† haute dimension (high-dimensional vector) ?

Le vecteur produit par le module Transformer est g√©n√©ralement de grande taille. Il a g√©n√©ralement **trois dimensions** :
- **Batch size::** nombre de s√©quences trait√©es √† la fois (2 dans notre exemple).
- **Longueur de la s√©quence:** longueur de la repr√©sentation num√©rique de la s√©quence (16 dans notre exemple).
- **Hidden size:** dimension vectorielle de chaque entr√©e du mod√®le.
Elle est dite **"high dimensional"** √† cause de la derni√®re valeur. La taille cach√©e peut √™tre tr√®s grande (768 est courant pour les petits mod√®les, et dans les grands mod√®les, elle peut atteindre 3072 ou plus).

Nous pouvons le constater si nous introduisons les entr√©es que nous avons pr√©trait√©es dans notre mod√®le :
```python
outputs = model(inputs)
print(outputs.last_hidden_state.shape)
```

Notez que les sorties des mod√®les ü§ó Transformers se comportent comme des **tuples nomm√©s ou des dictionnaires**. Vous pouvez acc√©der aux √©l√©ments par attributs (comme nous l'avons fait) ou par cl√© (outputs["last_hidden_state"]), ou m√™me par index si vous savez exactement o√π se trouve ce que vous cherchez (outputs[0]).


### T√™tes (heads) de mod√®les
Les t√™tes de mod√®le prennent en entr√©e le vecteur √† haute dimension des √©tats cach√©s et les projettent sur une autre dimension. Elles sont g√©n√©ralement compos√©es d'une ou de quelques **couches lin√©aires** :
![](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head.svg)

La sortie du mod√®le Transformer est envoy√©e directement √† la t√™te de mod√®le pour √™tre trait√©e.

Dans ce diagramme, le mod√®le est repr√©sent√© par sa couche d'embedding et les couches suivantes. 
- **La couche d'embedding** convertit chaque ID d'entr√©e dans l'entr√©e tokenis√©e en un vecteur qui repr√©sente le token associ√©.
- **Les couches suivantes** manipulent ces vecteurs √† l'aide du m√©canisme d'attention pour produire la repr√©sentation finale des phrases.

Il existe **de nombreuses architectures diff√©rentes disponibles dans ü§ó Transformers**, chacune √©tant con√ßue pour s'attaquer √† une t√¢che sp√©cifique. En voici une liste non exhaustive :
- *Model (retourne simplement les hidden states)
- *ForCausalLM
- *ForMaskedLM
- *ForMultipleChoice
- *ForQuestionAnswering
- *ForSequenceClassification
- *ForTokenClassification
- et bien d'autres biensur ü§ó

Pour notre exemple, nous aurons besoin d'un mod√®le avec une t√™te de classification de s√©quences (pour pouvoir classer les phrases comme positives ou n√©gatives). Nous n'utiliserons donc pas la classe TFAutoModel, mais TFAutoModelForSequenceClassification :
```python
from transformers import TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(inputs)
```

Si nous examinons maintenant la forme de nos r√©sultats, la dimensionnalit√© sera beaucoup plus faible : la t√™te du mod√®le prend en entr√©e les vecteurs √† haute dimension que nous avons vus pr√©c√©demment et produit des vecteurs contenant deux valeurs (une par √©tiquette) :
```python
print(outputs.logits.shape)
```

```
(2, 2)
```

Comme nous n'avons que deux phrases et deux √©tiquettes, le r√©sultat que nous obtenons de notre mod√®le est de **taille 2 x 2.**.


## Post-traitement des r√©sultats
Les valeurs que nous obtenons en sortie de notre mod√®le n'ont pas n√©cessairement de sens en elles-m√™mes. Jetons un coup d'≈ìil :
```python
print(outputs.logits)
```
```
<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
    array([[-1.5606991,  1.6122842],
           [ 4.169231 , -3.3464472]], dtype=float32)>
```

Notre mod√®le a pr√©dit [-1,5607, 1,6123] pour la premi√®re phrase et [ 4,1692, -3,3464] pour la seconde. Il ne s'agit pas de probabilit√©s mais de **logits**, les scores bruts et non normalis√©s produits par la derni√®re couche du mod√®le. Pour √™tre convertis en probabilit√©s, ils doivent passer 
par une couche **SoftMax** (tous les mod√®les ü§ó Transformers sortent les logits) :
```python
import tensorflow as tf

predictions = tf.math.softmax(outputs.logits, axis=-1)
print(predictions)
```
```
tf.Tensor(
[[4.01951671e-02 9.59804833e-01]
 [9.9945587e-01 5.4418424e-04]], shape=(2, 2), dtype=float32)
```

Nous voyons maintenant que le mod√®le a pr√©dit [0,0402, 0,9598] pour la premi√®re phrase et [0,9995, 0,0005] pour la seconde. Il s'agit de scores de probabilit√© reconnaissables.

Pour obtenir les √©tiquettes correspondant √† chaque position, nous pouvons inspecter l'attribut **`id2label` du mod√®le config** :
```python
model.config.id2label
```
```
{0: 'NEGATIVE', 1: 'POSITIVE'}
```

Nous pouvons maintenant conclure que le mod√®le a pr√©dit ce qui suit :

- **Premi√®re phrase : N√âGATIF : 0,0402, POSITIF : 0,9598**
- **Deuxi√®me phrase : N√âGATIVE : 0,9995, POSITIVE : 0,0005**

Nous avons reproduit avec succ√®s les trois √©tapes du pipeline : **pr√©-traitement avec des tokenizers, passage des entr√©es √† travers le mod√®le, et post-traitement!**. 
Prenons maintenant le temps d'approfondir chacune de ces √©tapes.
