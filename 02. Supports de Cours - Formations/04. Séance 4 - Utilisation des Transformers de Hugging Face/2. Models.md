# Modèles
Dans cette section, nous allons regarder de plus près la création et l'utilisation d'un modèle en utilisant la classe **TFAutoModel**.

La classe **TFAutoModel** est un simple wrapper (enveloppe si on peut le dire ainsi en francais) mais intelligente car elle peut automatiquement deviner l'architecture de modèle appropriée pour votre point de contrôle, et instancie ensuite un modèle avec cette architecture.

Cependant, si vous connaissez le type de modèle que vous souhaitez utiliser, vous pouvez utiliser directement la classe qui définit son architecture. 
Voyons comment cela fonctionne avec un modèle BERT.


## Création d'un Transformer
La première chose à faire pour initialiser un **modèle BERT** est de charger un objet de configuration :

```python
from transformers import BertConfig, TFBertModel

# Building the config
config = BertConfig()

# Building the model from the config
model = TFBertModel(config)
```

La configuration contient de nombreux attributs qui sont utilisés pour construire le modèle :
```
print(config)
```

```
BertConfig {
  [...]
  "hidden_size": 768,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  [...]
}
```

Vous devriez reconnaître certains de ces attributs :
- **hidden_size:** taille du vecteur **hidden_states**,
- **num_hidden_layers:** nombre de couches du modèle de transformateur.


### Différentes méthodes de chargement
La création d'un modèle à partir de la configuration par défaut l'initialise avec des valeurs aléatoires :

```python
from transformers import BertConfig, TFBertModel
config = BertConfig()
model = TFBertModel(config)


# Model is randomly initialized!
```

Le modèle peut être utilisé dans cet état, mais il produira un charabia ; il doit d'abord être entraîné. Nous pourrions entraîner le modèle à partir de zéro sur la tâche en question, mais comme on en a déjà longuement discuté, cela prendrait beaucoup de temps, nécessiterait beaucoup de données et aurait un impact non négligeable sur l'environnement. 
Pour éviter les efforts inutiles et redondants, il est impératif de pouvoir partager et réutiliser les modèles déjà formés.

Charger un modèle Transformer déjà entraîné est simple - nous pouvons le faire en utilisant la méthode **from_pretrained()** :
```python
from transformers import TFBertModel

model = TFBertModel.from_pretrained("bert-base-cased")
```

Comme vous l'avez vu précédemment, nous pouvons remplacer **TFBertModel** par la classe équivalente **TFAutoModel**.

Dans l'exemple de code ci-dessus, nous n'avons pas utilisé **BertConfig**, et avons chargé un modèle pré-entraîné via l'identifiant **bert-base-cased**. Il s'agit d'un **checkpoint** (point de contrôle) de modèle qui a été entraîné par les auteurs de BERT eux-mêmes ; vous pouvez trouver plus de détails à son sujet dans sa fiche de modèle.

Ce modèle est maintenant initialisé avec tous les poids du checkpoint. Il peut être utilisé directement pour l'inférence sur les tâches sur lesquelles il a été entraîné, et il peut également être affiné sur une nouvelle tâche. En s'entraînant avec des poids pré-entraînés plutôt qu'en partant de zéro, on peut rapidement obtenir de bons résultats.


### Sauvegarde des méthodes

Sauvegarder un modèle est aussi facile que d'en charger un - nous utilisons la méthode `save_pretrained()`, qui est analogue à la méthode **from_pretrained()** :
```python
model.save_pretrained("directory_on_my_computer")
```
Cela enregiste deux fichiers dans votre disque:
```
ls directory_on_my_computer

config.json tf_model.h5
```

- Fichier **config.json** : métadonnées et attributs nécessaires pour construire l'architecture du modèle.
- Fichier **tf_model.h5** : connu sous le nom de "state dictionary" (ou dictionnaire d'état), il contient tous les poids de votre modèle.

**Ces deux fichiers vont de pair ; la configuration est nécessaire pour connaître l'architecture de votre modèle, tandis que les poids du modèle sont les paramètres de votre modèle.


## Utiliser un modèle Transformer pour l'inférence
Les modèles Transformer ne peuvent traiter que des nombres - des nombres que le tokenizer génère.
Les **Tokenizers** peuvent se charger de couler les entrées dans les tenseurs du framework approprié, mais pour vous aider à comprendre ce qui se passe, nous allons examiner rapidement ce qui doit être fait avant d'envoyer les entrées au modèle.

Disons que nous avons deux séquences :
```
sequences = ["Hello!", "Cool.", "Nice!"]
```
The tokenizer converts these to vocabulary indices which are typically called **input IDs**. Each sequence is now a list of numbers!
```
encoded_sequences = [
    [101, 7592, 999, 102],
    [101, 4658, 1012, 102],
    [101, 3835, 999, 102],
]
```
Il s'agit d'une liste de séquences encodées : **une liste de listes**. Les tenseurs n'acceptent que des formes rectangulaires (pensez aux matrices). Ce « tableau » est déjà de forme rectangulaire, donc le convertir en tenseur est facile :
```
import tensorflow as tf

model_inputs = tf.constant(encoded_sequences)
```


### Utilisation des tenseurs comme entrées du modèle

L'utilisation des tenseurs avec le modèle est extrêmement simple - il suffit d'appeler le modèle avec les entrées :
```
output = model(model_inputs)
```

Bien que le modèle accepte un grand nombre d'arguments différents, seuls les ID d'entrée sont nécessaires. Nous expliquerons plus tard ce que font les autres arguments et quand ils sont nécessaires, mais nous devons d'abord examiner de plus près les tokenizers qui construisent les entrées qu'un modèle Transformer peut comprendre.
