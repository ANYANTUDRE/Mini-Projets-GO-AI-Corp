# Les tokenizers

**Les tokenizers** sont l'un des composants essentiels du pipeline NLP. Ils ont une seule fonction : **traduire le texte en données pouvant être traitées par le modèle** : **traduire le texte en données qui peuvent être traitées par le modèle**. 
Les modèles ne peuvent traiter que des nombres, l'objectif est donc de trouver un moyen de convertir les entrées textuelles brutes en nombres ou, disons, en représentations numériques significatives.

Examinons quelques **exemples d'algorithmes de tokenisation** :

## Basée sur les mots

**La tokenisation basée sur les mots est généralement très facile à mettre en place et à utiliser avec seulement quelques règles, et elle donne souvent des résultats décents. 
Par exemple, dans l'image ci-dessous, l'objectif est de diviser le texte brut en mots et de trouver une représentation numérique pour chacun d'entre eux :

![](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization.svg)

Il existe différentes manières de découper le texte. Par exemple, nous pourrions utiliser **whitespace** pour transformer le texte en mots en appliquant **la fonction split()** de Python :
```python
tokenized_text = "Jim Henson was a puppeteer".split()
print(tokenized_text)
```
```
['Jim', 'Henson', 'was', 'a', 'puppeteer']
```

Il existe également des variantes de tokenizers de mots qui ont des règles supplémentaires pour la ponctuation. 
Avec ce type de tokéniseur, nous pouvons obtenir des « vocabulaires » assez importants, un vocabulaire étant défini par le nombre total de tokens indépendants que nous avons dans notre corpus.

Chaque mot se voit attribuer un identifiant, commençant à 0 et allant jusqu'à la taille du vocabulaire. Le modèle utilise ces ID pour identifier chaque mot.

Si nous voulons couvrir complètement une langue avec un tokenizer basé sur les mots, nous devrons avoir un identifiant pour chaque mot de la langue, ce qui générera une énorme quantité de tokens. 
Par exemple, il y a plus de 500 000 mots dans la langue anglaise, donc pour construire une carte de chaque mot à un identifiant d'entrée, nous devrions garder la trace d'autant d'identifiants.

En outre, pour des mots tels que « dog » et « dogs » ou « run » et « running », le modèle n'aura initialement aucun moyen de savoir qu'ils sont similaires : il identifiera les deux mots comme n'étant pas liés. 

Enfin, nous avons besoin d'un jeton personnalisé pour représenter les mots qui ne font pas partie de notre vocabulaire. Il s'agit du jeton « inconnu », souvent représenté par **« [UNK] » ou « <unk> »**. 
C'est généralement un mauvais signe si vous voyez que le tokenizer produit beaucoup de ces jetons, car il n'a pas été capable de récupérer une représentation sensée d'un mot et vous perdez des informations en cours de route. 
L'objectif de l'élaboration du vocabulaire est de faire en sorte que le tokenizer génère le moins de mots possible dans le token inconnu.
Une façon de réduire le nombre de jetons inconnus est d'aller plus loin, en utilisant un tokenizer basé sur les caractères.


## Basé sur les caractères
Les tokenizers basés sur les caractères divisent le texte en caractères, plutôt qu'en mots. 

Cela présente deux avantages principaux :
- Le vocabulaire est beaucoup plus restreint.
- Il y a beaucoup moins de jetons hors vocabulaire (inconnus), puisque chaque mot peut être construit à partir de caractères.
Mais là encore, des questions se posent concernant les espaces et la ponctuation :

![](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization.svg)


Cette approche n'est pas non plus parfaite. Étant donné que la représentation est désormais basée sur des caractères plutôt que sur des mots, on pourrait dire que, intuitivement, elle est moins significative : chaque caractère ne signifie pas grand-chose en soi, alors que c'est le cas pour les mots. En chinois, par exemple, chaque caractère contient plus d'informations qu'un caractère dans une langue latine.

Un autre élément à prendre en compte est que nous nous retrouverons avec un très grand nombre de jetons à traiter par notre modèle : alors qu'un mot ne représenterait qu'un seul jeton avec un tokenizer basé sur les mots, il peut facilement se transformer en 10 jetons ou plus lorsqu'il est converti en caractères.

Pour obtenir le meilleur des deux mondes, nous pouvons utiliser une troisième technique qui combine les deux approches : **la tokenisation de sous-mots**.


## La tokenisation des sous-mots

**Les algorithmes de tokenisation par sous-mots** reposent sur le principe selon lequel les mots fréquemment utilisés ne doivent pas être divisés en sous-mots plus petits, mais les mots rares doivent être décomposés en sous-mots significatifs.

Par exemple, « annoyingly » peut être considéré comme un mot rare et pourrait être décomposé en « annoying » et « ly ». 
Ces deux mots sont susceptibles d'apparaître plus fréquemment en tant que sous-mots autonomes, tout en conservant le sens de « annoyingly » grâce au sens composite de « annoying » et « ly ».
Voici un exemple montrant comment un algorithme de tokenisation de sous-mots pourrait tokeniser la séquence « Let's do tokenization !
![](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword.svg)

Ces sous-mots finissent par fournir beaucoup de sens sémantique : par exemple, dans l'exemple ci-dessus, « tokenization » a été divisé en « token » et « ization », deux tokens qui ont un sens sémantique tout en étant peu encombrants (seuls deux tokens sont nécessaires pour représenter un mot long). 
Cela nous permet d'obtenir une couverture relativement bonne avec de petits vocabulaires, et presque aucun token inconnu.

Cette approche est particulièrement utile dans les langues agglutinantes comme le turc, où l'on peut former des mots complexes (presque) arbitrairement longs en enchaînant des sous-mots.


## Et plus encore !
Sans surprise, il existe de nombreuses autres techniques. En voici quelques-unes :

- **BPE au niveau de l'octet**, tel qu'utilisé dans GPT-2
- **WordPiece**, tel qu'utilisé dans BERT
- Les morceaux de phrase ou Unigramme**, utilisés dans plusieurs modèles multilingues.
Vous devriez maintenant avoir une connaissance suffisante du fonctionnement des tokenizers pour commencer à utiliser l'API.


## Chargement et sauvegarde

Le chargement et la sauvegarde des tokenizers sont aussi simples que pour les modèles. En fait, c'est basé sur les deux mêmes méthodes : **`from_pretrained()` et `save_pretrained()`**. Ces méthodes vont charger ou sauvegarder l'algorithme utilisé par le tokenizer (un peu comme l'architecture du modèle) ainsi que son vocabulaire (un peu comme les poids du modèle).

Le chargement du tokenizer BERT entraîné avec le même point de contrôle que BERT se fait de la même manière que le chargement du modèle, sauf que nous utilisons la classe BertTokenizer :

```python
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-cased")
```

Comme pour TFAutoModel, la classe **AutoTokenizer** récupère la classe de tokenizer appropriée dans la bibliothèque en fonction du nom du point de contrôle, et peut être utilisée directement avec n'importe quel point de contrôle :
```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

Nous pouvons maintenant utiliser le tokenizer comme indiqué dans la section précédente :
```python
tokenizer("Using a Transformer network is simple")
```

La sauvegarde d'un tokenizer est identique à celle d'un modèle :
```python
tokenizer.save_pretrained("directory_on_my_computer")
```

Nous reviendrons sur les **token_type_ids** au chapitre 3, et nous expliquerons la clé **attention_mask** un peu plus loin. Tout d'abord, voyons comment les **input_ids** sont générés. Pour ce faire, nous devons examiner les méthodes intermédiaires du tokenizer.


## Encodage
Traduire du texte en nombres est connu sous le nom de **encodage**. L'encodage se fait en deux étapes : la **kénisation**, suivie de la **conversion en ID d'entrée**.

- La première étape consiste à diviser le texte en mots (ou parties de mots, symboles de ponctuation, etc.), généralement appelés jetons. De nombreuses règles peuvent régir ce processus, c'est pourquoi nous devons instancier le tokenizer en utilisant le nom du modèle, afin de nous assurer que nous utilisons les mêmes règles que celles qui ont été utilisées lors du pré-entraînement du modèle.

- La deuxième étape consiste à convertir ces tokens en nombres, afin de pouvoir construire un tenseur à partir de ces tokens et de les transmettre au modèle. Pour ce faire, le tokenizer a **un vocabulaire**, qui est la partie que nous téléchargeons lorsque nous l'instancions avec la méthode `from_pretrained()`. Encore une fois, nous devons utiliser le même vocabulaire que celui utilisé lors du pré-entraînement du modèle.

Pour mieux comprendre les deux étapes, nous allons les explorer séparément.


### Tokénisation
Le processus de tokenisation est effectué par la méthode `tokenize()` du tokenizer :
```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

sequence = "Using a Transformer network is simple"
tokens = tokenizer.tokenize(sequence)

print(tokens)
```
The output of this method is a list of strings, or tokens:
```
['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']
```

**Ce tokenizer est un tokenizer de sous-mots** : il divise les mots jusqu'à ce qu'il obtienne des tokens qui peuvent être représentés par son vocabulaire.

### Des tokens aux ID d'entrée
La conversion en identifiants d'entrée est gérée par la méthode convert_tokens_to_ids() du tokenizer :
```python
ids = tokenizer.convert_tokens_to_ids(tokens)

print(ids)
```
```
[7993, 170, 13809, 23763, 2443, 1110, 3014]
```

Ces résultats, une fois convertis dans le tenseur de cadre approprié, peuvent alors être utilisés comme entrées dans un modèle, comme nous l'avons vu plus haut dans ce chapitre.

### Décodage
Le décodage se fait dans l'autre sens : à partir des indices de vocabulaire, nous voulons obtenir une chaîne de caractères. Cela peut être fait avec la méthode decode() comme suit :
```python
decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])
print(decoded_string)
```

```
Using a transformer network is simple
```

Notez que la méthode de décodage ne se contente pas de reconvertir les indices en tokens, mais regroupe également les tokens qui faisaient partie des mêmes mots pour produire une phrase lisible. Ce comportement sera extrêmement utile lorsque nous utiliserons des modèles qui prédisent un nouveau texte (soit un texte généré à partir d'une invite, soit pour des problèmes de séquence à séquence tels que la traduction ou le résumé).


Vous devriez maintenant comprendre les opérations atomiques qu'un tokenizer peut gérer : **la tokenisation, la conversion en ID et la reconversion des ID en chaîne de caractères**. 

Cependant, nous n'avons fait qu'effleurer la partie émergée de l'iceberg. Dans la section suivante, nous allons pousser notre approche jusqu'à ses limites et voir comment les dépasser.