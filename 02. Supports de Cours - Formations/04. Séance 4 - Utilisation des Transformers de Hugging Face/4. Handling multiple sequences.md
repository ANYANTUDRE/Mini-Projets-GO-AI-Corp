# Gestion de s√©quences multiples
Dans la section pr√©c√©dente, nous avons explor√© le cas d'utilisation le plus simple : **faire de l'inf√©rence sur une seule s√©quence de petite longueur**. Cependant, certaines questions se posent d√©j√† :

- Comment g√©rer des s√©quences multiples ? Des s√©quences de longueurs diff√©rentes ?
- Les indices de vocabulaire sont-ils les seules entr√©es qui permettent √† un mod√®le de bien fonctionner ?
- Existe-t-il une s√©quence trop longue ?
  
Voyons quels types de probl√®mes ces questions posent, et comment nous pouvons les r√©soudre en utilisant l'API **ü§ó Transformers**.


## Les mod√®les attendent un lot d'entr√©es

Dans l'exercice pr√©c√©dent, vous avez vu comment les s√©quences sont traduites en **listes de nombres**. Convertissons cette liste de nombres en tenseur et envoyons-la au mod√®le :

```python
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = tf.constant(ids)

# This line will fail.
model(input_ids)
```

```
InvalidArgumentError: Input to reshape is a tensor with 14 values, but the requested shape has 196 [Op:Reshape]
```

Pourquoi cela a-t-il √©chou√© ? "Nous avons suivi les √©tapes du pipeline dans la section 2.

Le probl√®me est que nous avons envoy√© **une seule s√©quence au mod√®le**, alors que **ü§ó les mod√®les Transformers s'attendent √† plusieurs phrases par d√©faut**. 
Ici, nous avons essay√© de faire tout ce que le tokenizer faisait dans les coulisses lorsque nous l'appliquions √† une s√©quence. 
Mais si vous regardez de pr√®s, vous verrez que le tokenizer ne s'est pas content√© de convertir la liste des ID en entr√©e en un tenseur, **il a ajout√© une dimension en plus :**

```python
tokenized_inputs = tokenizer(sequence, return_tensors="tf")
print(tokenized_inputs["input_ids"])
```

```
<tf.Tensor: shape=(1, 16), dtype=int32, numpy=
array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,
        12172,  2607,  2026,  2878,  2166,  1012,   102]], dtype=int32)>
```

Essayons √† nouveau et ajoutons une nouvelle dimension :

```python
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = tf.constant([ids])
print("Input IDs:", input_ids)

output = model(input_ids)
print("Logits:", output.logits)
```
Nous imprimons les ID d'entr√©e ainsi que les logits r√©sultants - voici la sortie :
```python
Input IDs: tf.Tensor(
[[ 1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026  2878
   2166  1012]], shape=(1, 14), dtype=int32)
Logits: tf.Tensor([[-2.7276208  2.8789377]], shape=(1, 2), dtype=float32)
```

Le **Batching** est l'action d'envoyer plusieurs phrases √† travers le mod√®le, en une seule fois. Si vous n'avez qu'une seule phrase, vous pouvez construire un lot avec une seule s√©quence :
```python
batched_ids = [ids, ids]
```
Il s'agit d'un lot de deux s√©quences identiques !
**L'utilisation de plusieurs s√©quences est aussi simple que la construction d'un lot avec une seule s√©quence**. Mais il y a un deuxi√®me probl√®me. 


## Padding the inputs

La liste de listes suivante ne peut pas √™tre convertie en tenseur :
```python
batched_ids = [
    [200, 200, 200],
    [200, 200]
]
```
Pour contourner ce probl√®me, nous utiliserons le **padding** pour donner une forme rectangulaire √† nos tenseurs. **Le padding** permet de s'assurer que toutes les phrases ont la m√™me longueur en ajoutant un mot sp√©cial appel√© padding token aux phrases ayant moins de valeurs.

L'ID du jeton de remplissage se trouve dans `tokenizer.pad_token_id`. Utilisons-le et envoyons nos deux phrases √† travers le mod√®le individuellement et ensemble :
```python
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[200, 200, 200]]
sequence2_ids = [[200, 200]]
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

print(model(tf.constant(sequence1_ids)).logits)
print(model(tf.constant(sequence2_ids)).logits)
print(model(tf.constant(batched_ids)).logits)
```

```
tf.Tensor([[ 1.5693678 -1.3894581]], shape=(1, 2), dtype=float32)
tf.Tensor([[ 0.5803005  -0.41252428]], shape=(1, 2), dtype=float32)
tf.Tensor(
[[ 1.5693681 -1.3894582]
 [ 1.3373486 -1.2163193]], shape=(2, 2), dtype=float32)
```
Il y a un probl√®me avec les logits dans nos pr√©dictions group√©es : **la deuxi√®me ligne devrait √™tre la m√™me que les logits pour la deuxi√®me phrase**, mais nous avons des valeurs compl√®tement diff√©rentes !

Cela s'explique par le fait que la caract√©ristique principale des mod√®les Transformer est l'**attention port√©e aux couches qui contextualisent chaque token**. Celles-ci prendront en compte les jetons de remplissage puisqu'elles s'int√©ressent √† tous les jetons d'une s√©quence. Pour obtenir le m√™me r√©sultat en faisant passer des phrases individuelles de longueurs diff√©rentes dans le mod√®le ou en faisant passer un lot avec les m√™mes phrases et le m√™me remplissage, nous devons dire √† ces couches d'attention d'ignorer les jetons de remplissage. Pour ce faire, nous utilisons un masque d'attention.


## Masques d'attention

**Les masques d'attention** sont des tenseurs ayant exactement la m√™me forme que le tenseur des identifiants d'entr√©e, **remplis de 0 et de 1** : Les **1 indiquent que les jetons correspondants doivent √™tre pris en compte**, et les **0 indiquent que les jetons correspondants ne doivent pas √™tre pris en compte** (c'est-√†-dire qu'ils doivent √™tre ignor√©s par les couches d'attention du mod√®le).

Compl√©tons l'exemple pr√©c√©dent par un masque d'attention :
```python
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

attention_mask = [
    [1, 1, 1],
    [1, 1, 0],
]

outputs = model(tf.constant(batched_ids), attention_mask=tf.constant(attention_mask))
print(outputs.logits)
```

```
tf.Tensor(
[[ 1.5693681  -1.3894582 ]
 [ 0.5803021  -0.41252586]], shape=(2, 2), dtype=float32)
```
Nous obtenons maintenant les m√™mes logits pour la deuxi√®me phrase du lot.

Remarquez que la derni√®re valeur de la deuxi√®me s√©quence est un ID de remplissage, qui est une valeur 0 dans le masque d'attention.


## S√©quences plus longues

Avec les mod√®les Transformer, **il y a une limite √† la longueur des s√©quences** que nous pouvons passer aux mod√®les. La plupart des mod√®les g√®rent des s√©quences allant jusqu'√† **512 ou 1024 tokens**, et se plantent lorsqu'on leur demande de traiter des s√©quences plus longues. Il existe deux solutions √† ce probl√®me :
- **Utiliser un mod√®le qui prend en charge des s√©quences plus longues.**
- **Tronquez vos s√©quences.**

Les mod√®les ont diff√©rentes longueurs de s√©quences support√©es, et certains sont sp√©cialis√©s dans le traitement de tr√®s longues s√©quences. Longformer en est un exemple, et LED en est un autre. Si vous travaillez sur une t√¢che qui n√©cessite des s√©quences tr√®s longues, nous vous recommandons de vous int√©resser √† ces mod√®les.

Sinon, nous vous recommandons de tronquer vos s√©quences en sp√©cifiant le param√®tre `max_sequence_length` :
```python
sequence = sequence[:max_sequence_length]
```
