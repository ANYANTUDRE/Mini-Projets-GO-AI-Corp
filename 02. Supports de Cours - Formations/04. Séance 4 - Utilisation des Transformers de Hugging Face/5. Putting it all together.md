# Mettre le tout en place

Dans les derni√®res sections, nous avons fait de notre mieux pour faire la plupart du travail √† la main. 
Nous avons explor√© le fonctionnement des tokenizers et examin√© **la tokenisation, la conversion en ID d'entr√©e, le padding, la troncature et les masques d'attention.**

Cependant, comme nous l'avons vu dans la section 2, l'API ü§ó Transformers peut g√©rer tout cela pour nous avec une fonction de haut niveau dans laquelle nous allons nous plonger ici. 
Lorsque vous appelez votre tokenizer directement sur la phrase, vous obtenez en retour des entr√©es qui sont pr√™tes √† passer dans votre mod√®le :

```python
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

Ici, la variable model_inputs contient tout ce qui est n√©cessaire au bon fonctionnement d'un mod√®le. 
Pour DistilBERT, cela inclut les identifiants d'entr√©e ainsi que le masque d'attention. Les autres mod√®les qui acceptent des entr√©es suppl√©mentaires seront √©galement produits par l'objet tokenizer.

Comme nous le verrons dans les exemples ci-dessous, cette m√©thode est tr√®s puissante. Tout d'abord, elle peut symboliser une seule s√©quence :
```python
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```
Il g√®re √©galement plusieurs s√©quences √† la fois, sans modification de l'API :
```python
sequences = ["I've been waiting for a HuggingFace course my whole life.",
             "So have I!" ]

model_inputs = tokenizer(sequences)
```

Le padding peu se faire en fontion de plusieurs objectifs :

```python
# Will pad the sequences up to the maximum sequence length
model_inputs = tokenizer(sequences, padding="longest")

# Will pad the sequences up to the model max length
# (512 for BERT or DistilBERT)
model_inputs = tokenizer(sequences, padding="max_length")

# Will pad the sequences up to the specified max length
model_inputs = tokenizer(sequences, padding="max_length", max_length=8)
```

Il peut √©galement tronquer des s√©quences :
```python
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# Will truncate the sequences that are longer than the model max length
# (512 for BERT or DistilBERT)
model_inputs = tokenizer(sequences, truncation=True)

# Will truncate the sequences that are longer than the specified max length
model_inputs = tokenizer(sequences, max_length=8, truncation=True)
```

L'objet tokenizer peut g√©rer la conversion en tenseurs de cadres sp√©cifiques, qui peuvent ensuite √™tre directement envoy√©s au mod√®le. Par exemple, dans l'exemple de code suivant, nous demandons au tokenizer de retourner les tenseurs des diff√©rents frameworks 
- pt" renvoie les tenseurs PyTorch,
- "tf" renvoie les tenseurs TensorFlow, et
- np" renvoie les tableaux NumPy :

```python
sequences = ["I've been waiting for a HuggingFace course my whole life.",
             "So have I!"]

# Returns PyTorch tensors
model_inputs = tokenizer(sequences, padding=True, return_tensors="pt")

# Returns TensorFlow tensors
model_inputs = tokenizer(sequences, padding=True, return_tensors="tf")

# Returns NumPy arrays
model_inputs = tokenizer(sequences, padding=True, return_tensors="np")
```

## Jetons sp√©ciaux

Si nous regardons les ID d'entr√©e renvoy√©s par le tokenizer, nous verrons qu'ils sont un peu diff√©rents de ce que nous avions pr√©c√©demment :
```python
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
print(model_inputs["input_ids"])

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)
```

Un jeton ID a √©t√© ajout√© au d√©but et un autre √† la fin. 
D√©codons les deux s√©quences d'ID ci-dessus pour comprendre de quoi il s'agit :
```python
print(tokenizer.decode(model_inputs["input_ids"]))
print(tokenizer.decode(ids))
```

```
"[CLS] i've been waiting for a huggingface course my whole life. [SEP]"
"i've been waiting for a huggingface course my whole life."
```

Le tokenizer a ajout√© le mot sp√©cial [CLS] au d√©but et le mot sp√©cial [SEP] √† la fin. 
C'est parce que le mod√®le a √©t√© pr√©-entra√Æn√© avec ces mots, de sorte que pour obtenir les m√™mes r√©sultats pour l'inf√©rence, nous devons les ajouter √©galement. 
Notez que certains mod√®les n'ajoutent pas de mots sp√©ciaux, ou en ajoutent d'autres ; les mod√®les peuvent √©galement ajouter ces mots sp√©ciaux uniquement au d√©but, ou uniquement √† la fin. 
Dans tous les cas, le tokenizer sait lesquels sont attendus et s'en occupera pour vous.


## Conclusion : Du tokenizer au mod√®le

Maintenant que nous avons vu toutes les √©tapes individuelles que l'objet tokenizer utilise lorsqu'il est appliqu√© sur des textes, voyons une derni√®re fois comment il peut g√©rer des s√©quences multiples (padding !), des s√©quences tr√®s longues (truncation !), et plusieurs types de tenseurs avec son API principale :

```python
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.",
             "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="tf")
output = model(**tokens)
```
